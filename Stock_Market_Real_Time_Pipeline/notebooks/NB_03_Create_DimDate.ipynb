{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col, explode, sequence, to_date, year, quarter, month, date_format, dayofweek\n","\n","print(\"Calculating date range from Gold data...\")\n","# 1. Get Min and Max dates dynamically from your actual data\n","date_range = spark.sql(\"SELECT MIN(date) as min_date, MAX(date) as max_date FROM StockMarket_Gold\").collect()[0]\n","min_date = date_range['min_date']\n","max_date = date_range['max_date']\n","\n","# Optional: Extend the range to ensure full years (e.g., start from Jan 1)\n","from datetime import date, timedelta\n","start_date = date(min_date.year, 1, 1)\n","end_date = date(max_date.year, 12, 31)\n","print(f\"Generating Date Dimension from {start_date} to {end_date}...\")\n","\n","# 2. Generate the sequence of dates\n","days_df = spark.sql(f\"SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day)) as date\")\n","\n","# 3. Add all standard calendar columns (The equivalent of your DAX)\n","dim_date_df = days_df.select(\n","    col(\"date\"),\n","    year(\"date\").alias(\"Year\"),\n","    quarter(\"date\").alias(\"QuarterNumber\"),\n","    date_format(\"date\", \"Q\").alias(\"Quarter\"),\n","    month(\"date\").alias(\"MonthNumber\"),\n","    date_format(\"date\", \"MMMM\").alias(\"Month\"),\n","    date_format(\"date\", \"MMM\").alias(\"MonthShort\"),\n","    dayofweek(\"date\").alias(\"DayOfWeekNumber\"), # Note: PySpark dayofweek usually 1=Sunday, 7=Saturday. Adjust if needed.\n","    date_format(\"date\", \"EEEE\").alias(\"DayOfWeek\"),\n","    date_format(\"date\", \"E\").alias(\"DayOfWeekShort\"),\n","    date_format(\"date\", \"yyyy-MM\").alias(\"YearMonth\"),\n","    date_format(\"date\", \"yyyy-'Q'Q\").alias(\"YearQuarter\")\n",")\n","\n","# 4. Write to Lakehouse as a physical Delta table\n","dim_date_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"DimDate\")\n","\n","print(\"Success! 'DimDate' table created in Lakehouse.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"23a34340-3f34-4e26-b079-93acf88b367c","normalized_state":"finished","queued_time":"2025-11-06T12:21:29.9159364Z","session_start_time":"2025-11-06T12:21:29.9168896Z","execution_start_time":"2025-11-06T12:21:41.0726435Z","execution_finish_time":"2025-11-06T12:22:16.064048Z","parent_msg_id":"1e87a30d-b6d6-4fb3-8de6-429f50c7fbf0"},"text/plain":"StatementMeta(, 23a34340-3f34-4e26-b079-93acf88b367c, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Calculating date range from Gold data...\nGenerating Date Dimension from 2020-01-01 to 2025-12-31...\nSuccess! 'DimDate' table created in Lakehouse.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4081219d-963e-48a2-bdc3-aa141f25c656"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"b499d455-0fbf-4bd4-afb9-0dd27764ee53"}],"default_lakehouse":"b499d455-0fbf-4bd4-afb9-0dd27764ee53","default_lakehouse_name":"LH_Stocks","default_lakehouse_workspace_id":"84759135-2722-4fac-b129-4906a6785cc3"}}},"nbformat":4,"nbformat_minor":5}